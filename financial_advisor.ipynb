{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b7f3e8a",
      "metadata": {
        "id": "7b7f3e8a"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --no-cache-dir \\\n",
        "    numpy==1.24.4 \\\n",
        "    pandas==2.2.2 \\\n",
        "    scipy==1.13.0 \\\n",
        "    scikit-learn==1.5.0 \\\n",
        "    matplotlib==3.9.0 \\\n",
        "    seaborn==0.13.2\n",
        "\n",
        "# 3Ô∏è‚É£ Install LangChain, OpenAI SDK, and AutoGen (latest stable versions)\n",
        "!pip install --upgrade --quiet \\\n",
        "    langchain langchain-openai langchain-community \\\n",
        "    openai python-dotenv openpyxl \\\n",
        "    autogen-agentchat \\\n",
        "    'autogen-ext[openai,azure]'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49bfaefc",
      "metadata": {
        "id": "49bfaefc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import sqlite3\n",
        "from google.colab import userdata\n",
        "import os\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_community.utilities import SQLDatabase\n",
        "from langchain_community.agent_toolkits import SQLDatabaseToolkit\n",
        "from langchain.agents import create_sql_agent\n",
        "from autogen_agentchat.agents import AssistantAgent\n",
        "from autogen_agentchat.agents import UserProxyAgent\n",
        "from autogen_agentchat.teams import SelectorGroupChat\n",
        "\n",
        "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
        "from autogen_agentchat.teams import RoundRobinGroupChat\n",
        "from autogen_agentchat.ui import Console\n",
        "import asyncio\n",
        "import numpy as np\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.svm import OneClassSVM\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec5654b4",
      "metadata": {
        "id": "ec5654b4"
      },
      "outputs": [],
      "source": [
        "api_key = userdata.get('openai_api_key')\n",
        "\n",
        "model_client = OpenAIChatCompletionClient(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    api_key=api_key,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e11c9bf",
      "metadata": {
        "id": "1e11c9bf"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(\n",
        "    model_name=\"gpt-4o-mini\",\n",
        "    openai_api_key=api_key,\n",
        "    temperature=0.1,  # Slightly higher temperature\n",
        "    max_tokens=1000   # Ensure enough tokens for responses\n",
        ")\n",
        "\n",
        "db = SQLDatabase.from_uri(\"sqlite:///mydb.db\")\n",
        "toolkit = SQLDatabaseToolkit(db=db, llm=llm)\n",
        "\n",
        "sql_agent = create_sql_agent(\n",
        "        llm=llm,\n",
        "        toolkit=toolkit,\n",
        "        verbose=True,\n",
        "        agent_type=\"openai-functions\",\n",
        "        handle_parsing_errors=True,\n",
        "        max_iterations=20,\n",
        "        early_stopping_method=\"generate\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def query_db(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Query the database using the SQL agent.\n",
        "\n",
        "    Args:\n",
        "        query (str): The user's question or request about the data\n",
        "\n",
        "    Returns:\n",
        "        str: The result from the database query\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Add context about the transactions table\n",
        "        enhanced_query = f\"{query}. Use the transactions table for this query.\"\n",
        "        result = sql_agent.run(enhanced_query)\n",
        "        return str(result)\n",
        "    except Exception as e:\n",
        "        return f\"Error querying database: {str(e)}\""
      ],
      "metadata": {
        "id": "kvY2Wm5_CMd6"
      },
      "id": "kvY2Wm5_CMd6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SQLAnomalyDetector:\n",
        "    def __init__(self, db_path=\"mydb.db\"):\n",
        "        self.db_path = db_path\n",
        "        self.conn = sqlite3.connect(db_path)\n",
        "        self.df = None\n",
        "\n",
        "    def load_data(self, query=None):\n",
        "        \"\"\"Load data from SQL database with improved date/time handling\"\"\"\n",
        "        if query is None:\n",
        "            query = \"SELECT * FROM transactions\"\n",
        "\n",
        "        self.df = pd.read_sql_query(query, self.conn)\n",
        "\n",
        "        # Print column information for debugging\n",
        "        print(f\"Loaded {len(self.df)} records\")\n",
        "        print(f\"Columns: {list(self.df.columns)}\")\n",
        "        if len(self.df) > 0:\n",
        "            print(f\"Column types: {self.df.dtypes.to_dict()}\")\n",
        "\n",
        "        # Handle your specific date/time format\n",
        "        datetime_created = False\n",
        "\n",
        "        # Your data has 'Date' and 'Time' columns in format: '01-Mar-22' and '12:00 AM'\n",
        "        if 'Date' in self.df.columns and 'Time' in self.df.columns:\n",
        "            try:\n",
        "                print(\"Attempting to combine Date and Time columns...\")\n",
        "                if len(self.df) > 0:\n",
        "                    print(f\"Sample Date: {self.df['Date'].iloc[0]}\")\n",
        "                    print(f\"Sample Time: {self.df['Time'].iloc[0]}\")\n",
        "\n",
        "                # Combine date and time strings\n",
        "                combined_strings = self.df['Date'].astype(str) + ' ' + self.df['Time'].astype(str)\n",
        "\n",
        "                # Try different parsing approaches for your specific format\n",
        "                try:\n",
        "                    # First try: assume format like '01-Mar-22 12:00 AM'\n",
        "                    self.df['DateTime'] = pd.to_datetime(combined_strings, format='%d-%b-%y %I:%M %p', errors='coerce')\n",
        "                except:\n",
        "                    try:\n",
        "                        # Second try: let pandas infer the format\n",
        "                        self.df['DateTime'] = pd.to_datetime(combined_strings, errors='coerce')\n",
        "                    except:\n",
        "                        # Third try: manual parsing\n",
        "                        self.df['DateTime'] = pd.to_datetime(combined_strings, infer_datetime_format=True, errors='coerce')\n",
        "\n",
        "                # Check if conversion was successful\n",
        "                valid_dates = self.df['DateTime'].notna().sum()\n",
        "                if valid_dates > 0:\n",
        "                    datetime_created = True\n",
        "                    print(f\"‚úì Successfully created DateTime from Date and Time columns\")\n",
        "                    print(f\"‚úì {valid_dates}/{len(self.df)} valid dates created\")\n",
        "                    print(f\"Date range: {self.df['DateTime'].min()} to {self.df['DateTime'].max()}\")\n",
        "                else:\n",
        "                    print(\"‚úó Failed to parse Date and Time columns\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error combining Date and Time: {e}\")\n",
        "\n",
        "        # Fallback: try to find other datetime columns\n",
        "        if not datetime_created:\n",
        "            print(\"‚ö† Could not create DateTime from Date/Time columns\")\n",
        "            print(\"‚ö† Time-based anomaly detection will be skipped\")\n",
        "\n",
        "        return self.df\n",
        "\n",
        "    def statistical_outliers(self, column, method='z_score', threshold=3):\n",
        "        \"\"\"Detect outliers using statistical methods\"\"\"\n",
        "        if self.df is None:\n",
        "            raise ValueError(\"Data not loaded. Call load_data() first.\")\n",
        "\n",
        "        if column not in self.df.columns:\n",
        "            raise ValueError(f\"Column '{column}' not found in data\")\n",
        "\n",
        "        data = self.df[column].dropna()\n",
        "\n",
        "        if len(data) == 0:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        # Convert to numeric if possible\n",
        "        try:\n",
        "            data = pd.to_numeric(data, errors='coerce').dropna()\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        if len(data) == 0:\n",
        "            print(f\"Warning: No numeric data found in column '{column}'\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        if method == 'z_score':\n",
        "            if data.std() == 0:\n",
        "                return pd.DataFrame()\n",
        "            z_scores = np.abs((data - data.mean()) / data.std())\n",
        "            outliers = z_scores > threshold\n",
        "\n",
        "        elif method == 'iqr':\n",
        "            Q1 = data.quantile(0.25)\n",
        "            Q3 = data.quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            if IQR == 0:\n",
        "                return pd.DataFrame()\n",
        "            lower_bound = Q1 - 1.5 * IQR\n",
        "            upper_bound = Q3 + 1.5 * IQR\n",
        "            outliers = (data < lower_bound) | (data > upper_bound)\n",
        "\n",
        "        elif method == 'modified_z_score':\n",
        "            median = data.median()\n",
        "            mad = np.median(np.abs(data - median))\n",
        "            if mad == 0:\n",
        "                return pd.DataFrame()\n",
        "            modified_z_scores = 0.6745 * (data - median) / mad\n",
        "            outliers = np.abs(modified_z_scores) > threshold\n",
        "\n",
        "        return self.df[self.df.index.isin(data[outliers].index)]\n",
        "\n",
        "    def isolation_forest_detection(self, columns, contamination=0.1):\n",
        "        \"\"\"Isolation Forest - Good for high-dimensional data\"\"\"\n",
        "        if self.df is None:\n",
        "            raise ValueError(\"Data not loaded. Call load_data() first.\")\n",
        "\n",
        "        # Check if columns exist\n",
        "        missing_cols = [col for col in columns if col not in self.df.columns]\n",
        "        if missing_cols:\n",
        "            raise ValueError(f\"Columns not found: {missing_cols}\")\n",
        "\n",
        "        # Select only numeric columns\n",
        "        numeric_cols = []\n",
        "        for col in columns:\n",
        "            try:\n",
        "                numeric_data = pd.to_numeric(self.df[col], errors='coerce')\n",
        "                if numeric_data.notna().sum() > 0:\n",
        "                    numeric_cols.append(col)\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        if not numeric_cols:\n",
        "            print(\"Warning: No numeric columns found for isolation forest\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        X = self.df[numeric_cols].copy()\n",
        "\n",
        "        # Convert to numeric and handle missing values\n",
        "        for col in numeric_cols:\n",
        "            X[col] = pd.to_numeric(X[col], errors='coerce')\n",
        "\n",
        "        X = X.dropna()\n",
        "\n",
        "        if len(X) == 0:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "        iso_forest = IsolationForest(contamination=contamination, random_state=42)\n",
        "        outliers = iso_forest.fit_predict(X_scaled)\n",
        "\n",
        "        anomaly_mask = outliers == -1\n",
        "        return self.df.loc[X.index[anomaly_mask]]\n",
        "\n",
        "    def financial_transaction_anomalies(self):\n",
        "        \"\"\"Detect financial transaction specific anomalies\"\"\"\n",
        "        if self.df is None:\n",
        "            raise ValueError(\"Data not loaded. Call load_data() first.\")\n",
        "\n",
        "        anomalies = {}\n",
        "\n",
        "        # Find amount columns with different possible names\n",
        "        amount_cols = []\n",
        "        for col in ['Amount', 'amount', 'AMOUNT', 'Cash Out', 'cash_out', 'Cash In', 'cash_in']:\n",
        "            if col in self.df.columns:\n",
        "                amount_cols.append(col)\n",
        "\n",
        "        # 1. Large cash withdrawals\n",
        "        for col in ['Cash Out', 'cash_out', 'withdrawal', 'debit']:\n",
        "            if col in self.df.columns:\n",
        "                try:\n",
        "                    cash_out_data = pd.to_numeric(self.df[col], errors='coerce')\n",
        "                    if cash_out_data.notna().sum() > 0 and (cash_out_data > 0).sum() > 0:\n",
        "                        large_withdrawals = self.statistical_outliers(col, method='iqr')\n",
        "                        if len(large_withdrawals) > 0:\n",
        "                            anomalies['large_withdrawals'] = large_withdrawals\n",
        "                        break\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "        # 2. Large cash deposits\n",
        "        for col in ['Cash In', 'cash_in', 'deposit', 'credit']:\n",
        "            if col in self.df.columns:\n",
        "                try:\n",
        "                    cash_in_data = pd.to_numeric(self.df[col], errors='coerce')\n",
        "                    if cash_in_data.notna().sum() > 0 and (cash_in_data > 0).sum() > 0:\n",
        "                        large_deposits = self.statistical_outliers(col, method='iqr')\n",
        "                        if len(large_deposits) > 0:\n",
        "                            anomalies['large_deposits'] = large_deposits\n",
        "                        break\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "        # 3. Unusual amounts\n",
        "        for col in ['Amount', 'amount', 'AMOUNT', 'transaction_amount']:\n",
        "            if col in self.df.columns:\n",
        "                try:\n",
        "                    unusual_amounts = self.statistical_outliers(col, method='iqr')\n",
        "                    if len(unusual_amounts) > 0:\n",
        "                        anomalies['unusual_amounts'] = unusual_amounts\n",
        "                    break\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "        # 4. Frequency anomalies - only if DateTime exists\n",
        "        if 'DateTime' in self.df.columns and self.df['DateTime'].notna().sum() > 0:\n",
        "            try:\n",
        "                daily_counts = self.df.groupby(self.df['DateTime'].dt.date).size()\n",
        "                if len(daily_counts) > 1:\n",
        "                    freq_threshold = daily_counts.mean() + 2 * daily_counts.std()\n",
        "                    high_freq_days = daily_counts[daily_counts > freq_threshold]\n",
        "                    if len(high_freq_days) > 0:\n",
        "                        high_freq_transactions = self.df[self.df['DateTime'].dt.date.isin(high_freq_days.index)]\n",
        "                        anomalies['high_frequency_days'] = high_freq_transactions\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Could not analyze frequency patterns: {e}\")\n",
        "\n",
        "        # 5. Round number bias\n",
        "        for col in amount_cols:\n",
        "            if col in self.df.columns:\n",
        "                try:\n",
        "                    amounts = pd.to_numeric(self.df[col], errors='coerce').dropna()\n",
        "                    if len(amounts) > 0:\n",
        "                        abs_amounts = amounts.abs()\n",
        "                        round_amounts = self.df[abs_amounts % 100 == 0]\n",
        "                        if len(round_amounts) > len(self.df) * 0.15:\n",
        "                            anomalies['round_number_bias'] = round_amounts\n",
        "                        break\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "        # 6. Late night transactions - only if DateTime exists\n",
        "        if 'DateTime' in self.df.columns and self.df['DateTime'].notna().sum() > 0:\n",
        "            try:\n",
        "                late_night_mask = (self.df['DateTime'].dt.hour >= 23) | (self.df['DateTime'].dt.hour <= 5)\n",
        "                late_night_transactions = self.df[late_night_mask]\n",
        "                if len(late_night_transactions) > 0:\n",
        "                    anomalies['late_night_transactions'] = late_night_transactions\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Could not analyze time patterns: {e}\")\n",
        "\n",
        "        return anomalies\n",
        "\n",
        "    def comprehensive_analysis(self, amount_col='Amount', cash_in_col='Cash In', cash_out_col='Cash Out'):\n",
        "        \"\"\"Run comprehensive anomaly detection analysis for financial transactions\"\"\"\n",
        "        if self.df is None:\n",
        "            raise ValueError(\"Data not loaded. Call load_data() first.\")\n",
        "\n",
        "        results = {}\n",
        "\n",
        "        print(\"üîç Running Comprehensive Financial Anomaly Detection Analysis...\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Check available columns based on your dataset\n",
        "        available_cols = []\n",
        "        column_mapping = {\n",
        "            'Amount': amount_col,\n",
        "            'Cash In': cash_in_col,\n",
        "            'Cash Out': cash_out_col\n",
        "        }\n",
        "\n",
        "        for display_name, col_name in column_mapping.items():\n",
        "            if col_name in self.df.columns:\n",
        "                available_cols.append(col_name)\n",
        "                print(f\"‚úì Found column: {col_name}\")\n",
        "            else:\n",
        "                print(f\"‚úó Column not found: {col_name}\")\n",
        "\n",
        "        if not available_cols:\n",
        "            print(f\"‚ùå None of the specified columns found in data\")\n",
        "            print(f\"Available columns: {list(self.df.columns)}\")\n",
        "            return results\n",
        "\n",
        "        # 1. Amount anomalies (both positive and negative values)\n",
        "        if amount_col in self.df.columns:\n",
        "            print(f\"\\n1. Amount Anomalies Analysis ({amount_col})\")\n",
        "            try:\n",
        "                # Z-score method\n",
        "                z_anomalies = self.statistical_outliers(amount_col, method='z_score')\n",
        "                results['amount_z_score'] = z_anomalies\n",
        "                print(f\"   Z-Score outliers: {len(z_anomalies)} anomalies\")\n",
        "\n",
        "                # IQR method\n",
        "                iqr_anomalies = self.statistical_outliers(amount_col, method='iqr')\n",
        "                results['amount_iqr'] = iqr_anomalies\n",
        "                print(f\"   IQR outliers: {len(iqr_anomalies)} anomalies\")\n",
        "\n",
        "                # Show some stats\n",
        "                amount_data = pd.to_numeric(self.df[amount_col], errors='coerce').dropna()\n",
        "                if len(amount_data) > 0:\n",
        "                    print(f\"   Amount range: {amount_data.min()} to {amount_data.max()}\")\n",
        "                    print(f\"   Amount mean: {amount_data.mean():.2f}, std: {amount_data.std():.2f}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"   Error analyzing amounts: {e}\")\n",
        "\n",
        "        # 2. Cash In anomalies (deposits)\n",
        "        if cash_in_col in self.df.columns:\n",
        "            print(f\"\\n2. Cash In Anomalies Analysis ({cash_in_col})\")\n",
        "            try:\n",
        "                # Only analyze non-zero cash in values\n",
        "                cash_in_data = pd.to_numeric(self.df[cash_in_col], errors='coerce')\n",
        "                non_zero_mask = cash_in_data > 0\n",
        "\n",
        "                if non_zero_mask.sum() > 0:\n",
        "                    cash_in_anomalies = self.statistical_outliers(cash_in_col, method='iqr')\n",
        "                    results['cash_in_anomalies'] = cash_in_anomalies\n",
        "                    print(f\"   Found {len(cash_in_anomalies)} large deposit anomalies\")\n",
        "                    print(f\"   Non-zero deposits: {non_zero_mask.sum()}/{len(self.df)} transactions\")\n",
        "                else:\n",
        "                    print(f\"   No cash deposits found in {cash_in_col}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"   Error analyzing cash in: {e}\")\n",
        "\n",
        "        # 3. Cash Out anomalies (withdrawals)\n",
        "        if cash_out_col in self.df.columns:\n",
        "            print(f\"\\n3. Cash Out Anomalies Analysis ({cash_out_col})\")\n",
        "            try:\n",
        "                # Only analyze non-zero cash out values\n",
        "                cash_out_data = pd.to_numeric(self.df[cash_out_col], errors='coerce')\n",
        "                non_zero_mask = cash_out_data > 0\n",
        "\n",
        "                if non_zero_mask.sum() > 0:\n",
        "                    cash_out_anomalies = self.statistical_outliers(cash_out_col, method='iqr')\n",
        "                    results['cash_out_anomalies'] = cash_out_anomalies\n",
        "                    print(f\"   Found {len(cash_out_anomalies)} large withdrawal anomalies\")\n",
        "                    print(f\"   Non-zero withdrawals: {non_zero_mask.sum()}/{len(self.df)} transactions\")\n",
        "                else:\n",
        "                    print(f\"   No cash withdrawals found in {cash_out_col}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"   Error analyzing cash out: {e}\")\n",
        "\n",
        "        # 4. Multi-column isolation forest\n",
        "        print(f\"\\n4. Multi-dimensional Analysis (Isolation Forest)\")\n",
        "        try:\n",
        "            numeric_cols = []\n",
        "            for col in available_cols:\n",
        "                try:\n",
        "                    test_data = pd.to_numeric(self.df[col], errors='coerce')\n",
        "                    if test_data.notna().sum() > 0:\n",
        "                        numeric_cols.append(col)\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            if len(numeric_cols) >= 2:\n",
        "                iso_anomalies = self.isolation_forest_detection(numeric_cols)\n",
        "                results['isolation_forest'] = iso_anomalies\n",
        "                print(f\"   Using columns: {numeric_cols}\")\n",
        "                print(f\"   Found {len(iso_anomalies)} multi-dimensional anomalies\")\n",
        "            else:\n",
        "                print(f\"   Need at least 2 numeric columns. Found: {numeric_cols}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   Error in isolation forest: {e}\")\n",
        "\n",
        "        # 5. Transaction pattern anomalies\n",
        "        print(f\"\\n5. Transaction Pattern Analysis\")\n",
        "        try:\n",
        "            # Category-based anomalies\n",
        "            if 'Category' in self.df.columns:\n",
        "                category_counts = self.df['Category'].value_counts()\n",
        "                print(f\"   Transaction categories: {len(category_counts)}\")\n",
        "                print(f\"   Most common: {category_counts.head(3).to_dict()}\")\n",
        "\n",
        "                # Find rare categories\n",
        "                rare_categories = category_counts[category_counts <= 2]\n",
        "                if len(rare_categories) > 0:\n",
        "                    rare_transactions = self.df[self.df['Category'].isin(rare_categories.index)]\n",
        "                    results['rare_category_transactions'] = rare_transactions\n",
        "                    print(f\"   Rare category transactions: {len(rare_transactions)}\")\n",
        "\n",
        "            # Mode-based anomalies\n",
        "            if 'Mode' in self.df.columns:\n",
        "                mode_counts = self.df['Mode'].value_counts()\n",
        "                print(f\"   Payment modes: {len(mode_counts)}\")\n",
        "                print(f\"   Most common: {mode_counts.head(3).to_dict()}\")\n",
        "\n",
        "                # Find rare modes\n",
        "                rare_modes = mode_counts[mode_counts <= 2]\n",
        "                if len(rare_modes) > 0:\n",
        "                    rare_mode_transactions = self.df[self.df['Mode'].isin(rare_modes.index)]\n",
        "                    results['rare_mode_transactions'] = rare_mode_transactions\n",
        "                    print(f\"   Rare mode transactions: {len(rare_mode_transactions)}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   Error analyzing patterns: {e}\")\n",
        "\n",
        "        # 6. Financial-specific anomalies\n",
        "        print(f\"\\n6. Financial-Specific Anomaly Detection\")\n",
        "        try:\n",
        "            fin_anomalies = self.financial_transaction_anomalies()\n",
        "            results['financial_anomalies'] = fin_anomalies\n",
        "\n",
        "            for anomaly_type, anomaly_data in fin_anomalies.items():\n",
        "                if isinstance(anomaly_data, pd.DataFrame):\n",
        "                    print(f\"   {anomaly_type.replace('_', ' ').title()}: {len(anomaly_data)} anomalies\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   Error in financial anomaly detection: {e}\")\n",
        "\n",
        "        # 7. Time-based analysis (if DateTime available)\n",
        "        if 'DateTime' in self.df.columns and self.df['DateTime'].notna().sum() > 0:\n",
        "            print(f\"\\n7. Time-Based Analysis\")\n",
        "            try:\n",
        "                # Hour-based patterns\n",
        "                hour_counts = self.df['DateTime'].dt.hour.value_counts().sort_index()\n",
        "                print(f\"   Time range: {hour_counts.index.min()}:00 to {hour_counts.index.max()}:00\")\n",
        "\n",
        "                # Find unusual timing\n",
        "                unusual_hours = hour_counts[hour_counts <= 1]  # Very few transactions\n",
        "                if len(unusual_hours) > 0:\n",
        "                    unusual_time_transactions = self.df[self.df['DateTime'].dt.hour.isin(unusual_hours.index)]\n",
        "                    results['unusual_time_transactions'] = unusual_time_transactions\n",
        "                    print(f\"   Unusual timing transactions: {len(unusual_time_transactions)}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"   Error in time analysis: {e}\")\n",
        "\n",
        "        return results"
      ],
      "metadata": {
        "id": "0Qpoe0uWM23_"
      },
      "id": "0Qpoe0uWM23_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_financial_data(query: str = None) -> str:\n",
        "    \"\"\"\n",
        "    Analyze financial data using the SQLAnomalyDetector\n",
        "\n",
        "    Args:\n",
        "        query (str): Optional SQL query to filter data\n",
        "\n",
        "    Returns:\n",
        "        str: Formatted analysis results\n",
        "    \"\"\"\n",
        "    try:\n",
        "        detector = SQLAnomalyDetector(\"mydb.db\")\n",
        "\n",
        "        # Load data\n",
        "        df = detector.load_data(query)\n",
        "\n",
        "        if df is None or len(df) == 0:\n",
        "            return \"No data found for analysis\"\n",
        "\n",
        "        # Run comprehensive analysis\n",
        "        results = detector.comprehensive_analysis()\n",
        "\n",
        "        # Format results for return\n",
        "        analysis_summary = f\"\"\"\n",
        "üìä COMPREHENSIVE FINANCIAL DATA ANALYSIS\n",
        "=========================================\n",
        "\n",
        "Dataset Overview:\n",
        "- Total records: {len(df)}\n",
        "- Date range: {df['Date'].min() if 'Date' in df.columns else 'N/A'} to {df['Date'].max() if 'Date' in df.columns else 'N/A'}\n",
        "- Columns: {list(df.columns)}\n",
        "\n",
        "Statistical Analysis:\n",
        "\"\"\"\n",
        "\n",
        "        for key, value in results.items():\n",
        "            if isinstance(value, dict):\n",
        "                if 'mean' in value:\n",
        "                    analysis_summary += f\"\\n{key.replace('_', ' ').title()}:\\n\"\n",
        "                    analysis_summary += f\"  Mean: {value['mean']:.2f}\\n\"\n",
        "                    analysis_summary += f\"  Std Dev: {value['std']:.2f}\\n\"\n",
        "                    analysis_summary += f\"  Range: {value['min']:.2f} to {value['max']:.2f}\\n\"\n",
        "                    analysis_summary += f\"  Median: {value['median']:.2f}\\n\"\n",
        "                    analysis_summary += f\"  Count: {value['count']}\\n\"\n",
        "                elif 'total_categories' in value:\n",
        "                    analysis_summary += f\"\\n{key.replace('_', ' ').title()}:\\n\"\n",
        "                    analysis_summary += f\"  Total Categories: {value['total_categories']}\\n\"\n",
        "                    analysis_summary += f\"  Top Categories: {value['top_categories']}\\n\"\n",
        "                    if value['rare_categories']:\n",
        "                        analysis_summary += f\"  Rare Categories: {value['rare_categories']}\\n\"\n",
        "                elif 'total_modes' in value:\n",
        "                    analysis_summary += f\"\\n{key.replace('_', ' ').title()}:\\n\"\n",
        "                    analysis_summary += f\"  Total Payment Modes: {value['total_modes']}\\n\"\n",
        "                    analysis_summary += f\"  Top Modes: {value['top_modes']}\\n\"\n",
        "                    if value['rare_modes']:\n",
        "                        analysis_summary += f\"  Rare Modes: {value['rare_modes']}\\n\"\n",
        "            elif isinstance(value, pd.DataFrame):\n",
        "                analysis_summary += f\"\\n{key.replace('_', ' ').title()}: {len(value)} anomalies detected\\n\"\n",
        "\n",
        "        return analysis_summary\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error in financial analysis: {str(e)}\"\n",
        "\n",
        "def finance_analysis(user_query: str) -> str:\n",
        "    \"\"\"\n",
        "    Combined finance analysis function that queries database and performs analysis\n",
        "\n",
        "    Args:\n",
        "        user_query (str): User's financial query\n",
        "\n",
        "    Returns:\n",
        "        str: Comprehensive financial analysis and recommendations\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"üìä Starting Financial Analysis...\")\n",
        "\n",
        "        # Step 1: Query database for transaction data\n",
        "        print(\"üîç Step 1: Querying transaction database...\")\n",
        "        db_query = \"SELECT * FROM transactions\"\n",
        "        db_result = query_db(db_query)\n",
        "\n",
        "        # Step 2: Perform comprehensive statistical analysis\n",
        "        print(\"üìà Step 2: Performing comprehensive statistical analysis...\")\n",
        "        analysis_result = analyze_financial_data()\n",
        "\n",
        "        # Step 3: Combine results and provide financial insights\n",
        "        print(\"üí° Step 3: Generating financial insights...\")\n",
        "\n",
        "        combined_analysis = f\"\"\"\n",
        "üè¶ COMPREHENSIVE FINANCIAL ANALYSIS REPORT\n",
        "===========================================\n",
        "\n",
        "USER QUERY: {user_query}\n",
        "\n",
        "{analysis_result}\n",
        "\n",
        "üí∞ FINANCIAL INSIGHTS & RECOMMENDATIONS:\n",
        "==========================================\n",
        "\n",
        "Based on the statistical analysis of your transaction data, here are the key findings and recommendations:\n",
        "\n",
        "1. **Spending Patterns**: Your transaction data shows various spending categories including Food, Travel, Personal expenses, and regular income sources.\n",
        "\n",
        "2. **Payment Preferences**: The analysis reveals your preferred payment methods and their usage patterns.\n",
        "\n",
        "3. **Anomaly Detection**: Statistical outliers have been identified that may represent unusual spending or income patterns worth reviewing.\n",
        "\n",
        "4. **Budget Recommendations**:\n",
        "   - Review high-value outlier transactions to ensure they align with your financial goals\n",
        "   - Consider the distribution of spending across categories for better budget allocation\n",
        "   - Monitor payment mode preferences for potential optimization\n",
        "\n",
        "5. **Risk Assessment**: The presence of anomalies suggests areas where you might want to implement spending controls or alerts.\n",
        "\n",
        "üéØ NEXT STEPS:\n",
        "- Review the statistical outliers identified in the analysis\n",
        "- Consider setting up budget categories based on your spending patterns\n",
        "- Monitor transactions that deviate significantly from your normal patterns\n",
        "- Implement alerts for unusual spending behaviors\n",
        "\n",
        "This analysis provides a foundation for better financial decision-making and budget management.\n",
        "\"\"\"\n",
        "\n",
        "        return combined_analysis\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error in finance analysis: {str(e)}\""
      ],
      "metadata": {
        "id": "WYjtFFShTLXG"
      },
      "id": "WYjtFFShTLXG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finance_agent = AssistantAgent(\n",
        "    name=\"finance_agent\",\n",
        "    model_client=model_client,\n",
        "    tools=[query_db],\n",
        "    system_message=\"\"\"You are a senior financial advisor and analyst. Your role is to:\n",
        "\n",
        "1. **Assess Query Complexity**: Determine if the user query requires basic data retrieval or complex statistical analysis\n",
        "2. **Handle Basic Queries**: For simple data requests, use query_db to get information and provide basic financial insights\n",
        "3. **Coordinate Complex Analysis**: For complex queries requiring statistical analysis, anomaly detection, or pattern recognition:\n",
        "   - First use query_db to understand the data structure\n",
        "   - Then request the data analyst to perform comprehensive analysis\n",
        "   - Wait for data analyst's statistical insights\n",
        "   - Combine their findings with additional data queries if needed\n",
        "4. **Provide Final Recommendations**: Give actionable financial advice based on SPECIFIC STATISTICAL FINDINGS\n",
        "\n",
        "**WHEN TO INVOLVE DATA ANALYST**:\n",
        "- User asks for \"analysis\", \"patterns\", \"trends\", \"anomalies\", or \"insights\"\n",
        "- Requests about spending behavior, risk assessment, or budget optimization\n",
        "- Questions requiring statistical analysis or data science techniques\n",
        "- Complex queries that need more than basic data retrieval\n",
        "\n",
        "**BASIC QUERIES** (handle yourself with query_db):\n",
        "- \"Show me recent transactions\"\n",
        "- \"What's my current balance?\"\n",
        "- \"List transactions by category\"\n",
        "- Simple data retrieval requests\n",
        "\n",
        "**COMPLEX QUERIES** (involve data analyst):\n",
        "- \"Analyze my spending patterns\"\n",
        "- \"Find unusual transactions\"\n",
        "- \"What are my financial trends?\"\n",
        "- \"Provide budget recommendations\"\n",
        "\n",
        "**CRITICAL - USE SPECIFIC DATA FROM ANALYSIS**:\n",
        "When data analyst provides statistical insights, use the ACTUAL NUMBERS and SPECIFIC FINDINGS:\n",
        "- Reference exact amounts, percentages, and counts from their analysis\n",
        "- Quote specific statistics like \"Your average spending is Rs.X with Rs.Y standard deviation\"\n",
        "- Use actual category breakdowns like \"Food represents 270 out of 564 transactions (48%)\"\n",
        "- Reference specific anomaly counts and what they mean financially\n",
        "- Base recommendations on the actual data patterns found\n",
        "\n",
        "**IMPORTANT**:\n",
        "- After data analyst provides statistical insights, interpret the SPECIFIC NUMBERS and provide targeted recommendations\n",
        "- Don't provide generic advice - use the actual statistical findings\n",
        "- Always provide financial interpretation and recommendations based on real data\n",
        "- Reference specific anomalies, spending patterns, and statistical measures\n",
        "\n",
        "**YOUR FINAL RESPONSE SHOULD INCLUDE**:\n",
        "- Interpretation of specific statistical findings (using actual numbers)\n",
        "- Targeted recommendations based on the real spending patterns found\n",
        "- Risk assessment based on actual anomaly counts and patterns\n",
        "- Specific next steps based on the data analysis results\"\"\",\n",
        "    reflect_on_tool_use=True,\n",
        ")\n",
        "\n",
        "data_analyst = AssistantAgent(\n",
        "    name=\"data_analyst\",\n",
        "    model_client=model_client,\n",
        "    tools=[analyze_financial_data],\n",
        "    system_message=\"\"\"You are a senior data analyst specializing in financial data analysis. Your role is to:\n",
        "\n",
        "1. **Receive Analysis Requests**: Finance agent will request statistical analysis for complex queries\n",
        "2. **Perform Comprehensive Analysis**: Use analyze_financial_data function to run detailed statistical analysis\n",
        "3. **Provide Statistical Insights**: Return detailed findings with ACTUAL NUMBERS and SPECIFIC DATA from the analysis\n",
        "\n",
        "**YOUR ANALYSIS MUST INCLUDE ACTUAL VALUES FROM THE ANALYSIS RESULT**:\n",
        "- Use the specific numbers returned by the analyze_financial_data function\n",
        "- Quote exact statistics (mean, median, std dev, ranges) from the analysis\n",
        "- Reference specific category counts and payment mode statistics\n",
        "- Include actual anomaly counts and outlier details\n",
        "- Provide real insights based on the returned data, not generic placeholders\n",
        "\n",
        "**EXAMPLE OF PROPER ANALYSIS**:\n",
        "\"Based on the analysis results:\n",
        "- Amount statistics: Mean: -0.49, Std Dev: 3826.63, Range: -61100 to 65044\n",
        "- Transaction categories: 14 total, with Food (270 transactions), Drink (119), Travel (39) as top categories\n",
        "- Payment modes: Google Pay (436 transactions), Cash (56), Paytm (30)\n",
        "- Anomalies detected: 63 IQR outliers, 55 multi-dimensional anomalies, 38 cash withdrawal anomalies\"\n",
        "\n",
        "**IMPORTANT**:\n",
        "- NEVER use placeholders like \"(Specify Mean)\" or \"(Provide descriptive stats here)\"\n",
        "- ALWAYS extract and report the actual numerical values from the analysis results\n",
        "- Base your insights on the real data returned by the analyze_financial_data function\n",
        "- Be specific about what the numbers mean statistically\n",
        "\n",
        "**RESPONSE FORMAT**:\n",
        "After analysis, provide comprehensive statistical findings with ACTUAL DATA and conclude with:\n",
        "\"Finance agent, please interpret these SPECIFIC statistical insights and provide financial recommendations to the user.\"\n",
        "\"\"\",\n",
        "    reflect_on_tool_use=True,\n",
        ")\n",
        "\n",
        "from autogen_agentchat.teams import SelectorGroupChat\n",
        "\n",
        "group_chat = SelectorGroupChat(\n",
        "    participants=[finance_agent, data_analyst],\n",
        "    model_client=model_client,\n",
        ")\n",
        "\n",
        "async def run_finance_analysis(user_query: str):\n",
        "    \"\"\"\n",
        "    Run the two-agent financial analysis workflow with live conversation display\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"üöÄ Starting Two-Agent Financial Analysis...\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        task = f\"\"\"\n",
        "USER QUERY: {user_query}\n",
        "\n",
        "WORKFLOW INSTRUCTIONS:\n",
        "1. Finance Agent: Assess if this is a basic query or requires complex analysis\n",
        "2. For BASIC queries: Finance agent uses query_db and provides simple insights\n",
        "3. For COMPLEX queries:\n",
        "   - Finance agent requests data analyst to perform statistical analysis\n",
        "   - Data analyst uses analyze_financial_data function and returns statistical insights\n",
        "   - Finance agent MUST interpret the statistical insights and provide final financial recommendations\n",
        "\n",
        "GOAL: Provide comprehensive financial analysis with actionable recommendations.\n",
        "\"\"\"\n",
        "\n",
        "        # Create an async iterator to show conversation in real-time\n",
        "        async for message in group_chat.run_stream(task=task):\n",
        "            if hasattr(message, 'source') and hasattr(message, 'content'):\n",
        "                if message.source == 'finance_agent':\n",
        "                    print(f\"\\nüè¶ FINANCE AGENT:\")\n",
        "                    print(\"-\" * 40)\n",
        "                    print(f\"{message.content}\")\n",
        "                    print(\"-\" * 40)\n",
        "                elif message.source == 'data_analyst':\n",
        "                    print(f\"\\nüìä DATA ANALYST:\")\n",
        "                    print(\"-\" * 40)\n",
        "                    print(f\"{message.content}\")\n",
        "                    print(\"-\" * 40)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"‚úÖ Analysis Complete!\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error in financial analysis: {e}\")\n",
        "        return False\n",
        "\n",
        "async def main():\n",
        "    test_queries = [\n",
        "        \"Analyze my spending patterns and provide budget recommendations\",  # Complex\n",
        "        \"Show me my recent transactions\",  # Basic\n",
        "        \"What are my financial trends and any anomalies?\",  # Complex\n",
        "        \"List my transactions by category\",  # Basic\n",
        "        \"Perform comprehensive analysis of my financial data\",  # Complex\n",
        "        \"What's my current balance trend?\"  # Basic\n",
        "    ]\n",
        "\n",
        "    query = input(\"What would you like to know about your financial data? \") or test_queries[0]\n",
        "\n",
        "    success = await run_finance_analysis(query)\n",
        "\n",
        "    if not success:\n",
        "        print(\"‚ùå Analysis failed. Please check your setup and try again.\")\n",
        "\n",
        "await main()"
      ],
      "metadata": {
        "id": "Ch3y3OxH5vOH"
      },
      "id": "Ch3y3OxH5vOH",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}